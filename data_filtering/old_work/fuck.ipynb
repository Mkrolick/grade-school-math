{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sent2vec.vectorizer import Vectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from dataset import get_examples\n",
    "from word_frequency import *\n",
    "from pathlib import Path\n",
    "from utils import *\n",
    "import sys\n",
    "\n",
    "\n",
    "#train sent2vec\n",
    "# load sentence to vec mode from wiki_unigrams.bin\n",
    "\n",
    "# Load sent2vec vectorizer\n",
    "# using bert-base-uncased\n",
    "# vectorizer.bert(\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of words to remove 331013\n",
      "Word Vocab Space 2320\n",
      "7473 train examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train sent2vec\n",
    "# load sentence to vec mode from wiki_unigrams.bin\n",
    "\n",
    "# Load sent2vec vectorizer\n",
    "# using bert-base-uncased\n",
    "# vectorizer.bert(\"bert-base-uncased\")\n",
    "vectorizer = Vectorizer()\n",
    "\n",
    "# my own multi dimensional list iterator :)\n",
    "\n",
    "def max_iterator(iterator_list, previous_index = []):\n",
    "    if len(iterator_list) > 0: \n",
    "        for i in range(iterator_list[-1]):\n",
    "            max_iterator(iterator_list[:-1], [i] + previous_index)\n",
    "    elif len(iterator_list) == 0:\n",
    "        do_thing(previous_index)\n",
    "        print(previous_index)\n",
    "\n",
    "def do_thing(previous_index, sub_list):\n",
    "    sub_list.append()\n",
    "            \n",
    "\n",
    "# make this generalized\n",
    "summed_percentile = 0.68\n",
    "words_to_remove = list(subset_percentile(summed_percentile)[\"word\"])\n",
    "count_word_to_remove = len(words_to_remove)\n",
    "\n",
    "print(\"length of words to remove\", count_word_to_remove)\n",
    "print(\"Word Vocab Space\", word_space() - count_word_to_remove)\n",
    "\n",
    "\n",
    "#sys.exit()\n",
    "\n",
    "training_exaples = get_examples(\"train\", jypter=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions = [example[\"question\"] for example in training_exaples]\n",
    "\n",
    "\n",
    "sentences = tokenize_sentence(questions[0])\n",
    "    \n",
    "sentence = sentences[1]\n",
    "words = filter_words(sentence) \n",
    "\n",
    "# remove words that are not in the data\n",
    "valid_words = [x for x in words if word_in_data(x)]\n",
    "\n",
    "word_to_remove = [words for words in words if words in words_to_remove]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'many',\n",
       " 'clips',\n",
       " 'did',\n",
       " 'natalia',\n",
       " 'sell',\n",
       " 'altogether',\n",
       " 'in',\n",
       " 'april',\n",
       " 'and',\n",
       " 'may']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clips', 'natalia', 'altogether']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clips': [], 'natalia': [], 'altogether': ['raw', 'all', 'completely', 'entirely', 'right', 'totally', 'whole', 'wholly']}\n"
     ]
    }
   ],
   "source": [
    "replacement_tuples = {}\n",
    "\n",
    "for word in word_to_remove:\n",
    "    # for word, appends valid synonym\n",
    "\n",
    "    replacement_tuples[word] = synonym_better(word)[0]\n",
    "    # dict structure is {word: [synonym, synonym, synonym]}\n",
    "\n",
    "# make possible matrix of all possible combinations of sentences\n",
    "\n",
    "# find where the word is in the sentence including uppercase and lowercase combinations\n",
    "words = replacement_tuples.keys()\n",
    "\n",
    "#index_start = [0 for x in range(len(words))]\n",
    "print(replacement_tuples)\n",
    "\n",
    "index_end = [len(replacement_tuples[x]) for x in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 8]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 8]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(previous_index, sentence, replacement_tuples):\n",
    "    # key, replacement_tuples[index]\n",
    "    word_subs = [(replacement_tuple[0], replacement_tuple[1]) for index, replacement_tuple in enumerate(replacement_tuples.items())]\n",
    "    paired_value = [(replacement_tuple[0], replacement_tuple[1][previous_index[index]]) for index, replacement_tuple in enumerate(word_subs) if replacement_tuple[1] != []]\n",
    "\n",
    "    #print(previous_index)\n",
    "    #print(paired_value)\n",
    "\n",
    "    for word, replacement in paired_value:\n",
    "        # begging and ending of a sentence\n",
    "        sentence_indicies = replacement_index(sentence, word, \"a\")\n",
    "\n",
    "        if sentence_indicies[0] == 0:\n",
    "            sentence = replacement[:1].upper() + replacement[1:] + sentence[sentence_indicies[1]:]\n",
    "        else:\n",
    "            sentence = sentence[:sentence_indicies[0]] + replacement + sentence[sentence_indicies[1]:]\n",
    "        \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_iterator(iterator_list, previous_index = [], function_args = None):\n",
    "    sentences = []\n",
    "    if len(iterator_list) > 0: \n",
    "        # if the last index is 0, then we don't want to iterate over it and just call the function\n",
    "        if (iterator_list[-1] == 0):\n",
    "            sentence = max_iterator(iterator_list[:-1], [0] + previous_index, function_args)                                                                                                                                                                                                                                                              \n",
    "            sentences.append(sentence)\n",
    "\n",
    "        for i in range(iterator_list[-1]):\n",
    "            sentence = max_iterator(iterator_list[:-1], [i] + previous_index, function_args)                                                                                                                                                                                                                                                              \n",
    "            sentences.append(sentence)\n",
    "        print(iterator_list)\n",
    "        return sentences\n",
    "    elif len(iterator_list) == 0:\n",
    "        # start building the sentence\n",
    "        return make_sentence(previous_index, function_args[0], function_args[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0, 0]\n",
      "[0]\n",
      "[0, 0]\n",
      "[0]\n",
      "[0, 0]\n",
      "[0]\n",
      "[0, 0]\n",
      "[0]\n",
      "[0, 0]\n",
      "[0]\n",
      "[0, 0]\n",
      "[0]\n",
      "[0, 0]\n",
      "[0]\n",
      "[0, 0]\n",
      "[0, 0, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['How many clips did Natalia sell raw in April and May?\\n',\n",
       "       'How many clips did Natalia sell all in April and May?\\n',\n",
       "       'How many clips did Natalia sell completely in April and May?\\n',\n",
       "       'How many clips did Natalia sell entirely in April and May?\\n',\n",
       "       'How many clips did Natalia sell right in April and May?\\n',\n",
       "       'How many clips did Natalia sell totally in April and May?\\n',\n",
       "       'How many clips did Natalia sell whole in April and May?\\n',\n",
       "       'How many clips did Natalia sell wholly in April and May?\\n'],\n",
       "      dtype='<U61')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_unflattened = max_iteratora([0,0,8], previous_index=[], function_args = [sentence, replacement_tuples])\n",
    "import numpy as np\n",
    "\n",
    "sentences_unflattened = np.asarray(sentences_unflattened)\n",
    "sentences_unflattened.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "708b99c6c03df114470a5a9bc4e0a0e7696e9aad15535a2bf14f9a2e75bddcbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
